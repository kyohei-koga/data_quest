{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, let's use the logistic regression model we fit in the last mission to predict the class labels for each observation in the dataset and add these labels to the Dataframe in a separate column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    598\n",
      "1     46\n",
      "Name: predicted_label, dtype: int64\n",
      "   admit       gpa         gre  predicted_label\n",
      "0      0  3.177277  594.102992                0\n",
      "1      0  3.412655  631.528607                0\n",
      "2      0  2.728097  553.714399                0\n",
      "3      0  3.093559  551.089985                0\n",
      "4      0  3.141923  537.184894                0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "admissions = pd.read_csv(\"admission.csv\")\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(admissions[[\"gpa\"]],admissions[\"admit\"])\n",
    "labels = model.predict(admissions[[\"gpa\"]])\n",
    "admissions[\"predicted_label\"] = labels\n",
    "print(admissions[\"predicted_label\"].value_counts())\n",
    "print(admissions.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy\n",
    "The simplest way to determine the effectiveness of a classification model is prediction accuracy. Accuracy helps us answer the question:\n",
    "- What fraction of the predictions were correct (actual label matched predicted label)?\n",
    "\n",
    "### Accuracy = # of Correctly Predicted / # of Observations\n",
    "\n",
    "To decide who gets admitted, we set a threshold and accept all of the students where their computed probability exceeds that threshold. This threshold is called the discrimination threshold and scikit-learn sets it to 0.5 by default when predicting labels. If the predicted probability is greater than 0.5, the label for that observation is 1. If it is instead less than 0.5, the label for that observation is 0.\n",
    "\n",
    "An accuracy of 1.0 means that the model predicted 100% of admissions correctly for the given discrimination threshold. An accuracy of 0.2 means that the model predicted 20% of the admissions correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   actual_label       gpa         gre  predicted_label\n",
      "0             0  3.177277  594.102992                0\n",
      "1             0  3.412655  631.528607                0\n",
      "2             0  2.728097  553.714399                0\n",
      "3             0  3.093559  551.089985                0\n",
      "4             0  3.141923  537.184894                0\n",
      "0.645962732919\n"
     ]
    }
   ],
   "source": [
    "admissions.rename(columns={\"admit\":\"actual_label\"},inplace=True) #change the column name\n",
    "matches = admissions[\"actual_label\"] == admissions[\"predicted_label\"]\n",
    "correct_predictions = admissions[matches == True]\n",
    "print(correct_predictions.head(5))\n",
    "accuracy = correct_predictions.shape[0] / float(admissions.shape[0])\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classification outcomes\n",
    "To start, let's discuss the 4 different outcomes of a binary classification model:\n",
    "\n",
    "- True Postive - The model correctly predicted that the student would be admitted.\n",
    "- True Negative - The model correctly predicted that the student would be rejected.\n",
    "- False Positive - The model incorrectly predicted that the student would be admitted even though the student was actually rejected.\n",
    "- False Negative - The model incorrectly predicted that the student would be rejected even though the student was actually admitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 31 385\n"
     ]
    }
   ],
   "source": [
    "true_positives = admissions[(admissions[\"actual_label\"] == 1) & (admissions[\"predicted_label\"] == 1)].shape[0]\n",
    "true_neagetives = admissions[(admissions[\"actual_label\"] == 0) & (admissions[\"predicted_label\"] == 0)].shape[0]\n",
    "print true_positives,true_neagetives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitivity\n",
    "Let's now look at a few measures that are much more insightful than simple accuracy. Let's start with sensitivity:\n",
    "- Sensitivity or True Postive Rate - The proportion of applicants that were correctly admitted:\n",
    "### TPR = True positives / (True Positives + False Negatives)\n",
    "\n",
    "this measure helps us answer the question:\n",
    "- How effective is this model at identifying positive outcomes?\n",
    "\n",
    "We want a highly sensitive model that is able to \"catch\" all of the positive cases (in this case, the positive case is a patient with cancer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.127049180328\n"
     ]
    }
   ],
   "source": [
    "false_negatives = admissions[(admissions[\"actual_label\"] == 1) & (admissions[\"predicted_label\"] == 0)].shape[0]\n",
    "sensitivity = true_positives / float(true_positives + false_negatives)\n",
    "print sensitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specificity\n",
    "Let's now learn about specificity:\n",
    "- Specificity or True Negative Rate - The proportion of applicants that were correctly rejected:\n",
    "### TNR = True Negative / (False Positives + True Negatives)\n",
    "\n",
    "This heps us answer the questions:\n",
    "- How effective is this model at identifying negative outcomes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9625\n"
     ]
    }
   ],
   "source": [
    "false_positives = admissions[(admissions[\"actual_label\"] == 0) & (admissions[\"predicted_label\"] == 1)].shape[0]\n",
    "specificity = true_neagetives / float(true_neagetives + false_positives)\n",
    "print specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
