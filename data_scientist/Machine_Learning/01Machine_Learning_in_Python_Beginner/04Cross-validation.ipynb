{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to validation\n",
    "If we only evaluate a classifier's effectiveness on the data it was trained on, we can run into overfitting, where the classifier only performs well on the training but doesn't generalize to future data.\n",
    "\n",
    "To test a classifier's generalizability, or its ability to provide accurate predictions on data it wasn't trained on, we use cross-validation techniques. Cross-validation involves splitting historical data into:\n",
    "\n",
    "- a training set -- which we use to train the classifer,\n",
    "- a test set -- which we use to evaluate the classifier's effectiveness using various measures.\n",
    "\n",
    "Cross-validation is an important step that should be utilized after training any kind of machine learning model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        gpa         gre  actual_label\n",
      "0  3.177277  594.102992             0\n",
      "1  3.412655  631.528607             0\n",
      "2  2.728097  553.714399             0\n",
      "3  3.093559  551.089985             0\n",
      "4  3.141923  537.184894             0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "admissions = pd.read_csv(\"admission.csv\")\n",
    "admissions[\"actual_label\"] = admissions[\"admit\"]\n",
    "admissions = admissions.drop(\"admit\",axis=1)\n",
    "\n",
    "print(admissions.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Holdout validation\n",
    "The simplest technique is called holdout validation, which involves randomly splitting our dataset into a training data and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          gpa         gre  actual_label\n",
      "188  3.229678  563.682408             0\n",
      "106  3.465695  578.830098             0\n",
      "465  3.631865  561.211942             1\n",
      "527  3.269801  684.722621             1\n",
      "410  3.022044  614.554527             1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "shuffled_admissions = admissions.loc[np.random.permutation(len(admissions))]\n",
    "train = shuffled_admissions[:515]\n",
    "test = shuffled_admissions[515:]\n",
    "\n",
    "print(shuffled_admissions.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy\n",
    "Recall that accuracy helps us answer the question:\n",
    "\n",
    "- What fraction of the predictions were correct (actual label matched predicted label)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.612403100775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kyohei/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "shuffled_index = np.random.permutation(admissions.index)\n",
    "shuffled_admissions = admissions.loc[shuffled_index]\n",
    "train = shuffled_admissions[:515]\n",
    "test = shuffled_admissions[515:]\n",
    "logical_model = LogisticRegression()\n",
    "logical_model.fit(train[[\"gpa\"]],train[\"actual_label\"])\n",
    "test[\"predicted_label\"] = logical_model.predict(test[['gpa']])\n",
    "\n",
    "matches = test['predicted_label'] == test['actual_label']\n",
    "correct_predictions = test[matches]\n",
    "accuracy = len(correct_predictions) / float(len(test))\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitivity and specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0869565217391\n",
      "0.903614457831\n"
     ]
    }
   ],
   "source": [
    "true_positives = test[(test[\"predicted_label\"] == 1) & (test[\"actual_label\"] == 1)].shape[0]\n",
    "false_negatives = test[(test[\"predicted_label\"] == 0) & (test['actual_label'] == 1)].shape[0]\n",
    "sensitivity = true_positives / float(true_positives + false_negatives)\n",
    "print sensitivity\n",
    "\n",
    "false_positives = test[(test[\"predicted_label\"] == 1) & (test['actual_label'] == 0)].shape[0]\n",
    "true_negatives = test[(test[\"predicted_label\"] == 0) & (test['actual_label'] == 0)].shape[0]\n",
    "specifity = true_negatives / float(true_negatives + false_positives)\n",
    "print specifity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# False positive rate\n",
    " If the probability value is larger than 50%, the predicted label is 1 and if it's less than 50%, the predictd label is 0. For most problems, however, 50% is not the optimal discrimination threshold. We need a way to vary the threshold and compute the measures at each threshold. Then, depending on the measure we want to optimize, we can find the appropriate threshold to use for predictions.\n",
    " \n",
    " ### FPR = False Positives / (False Positives + True Negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC Curve\n",
    "We can vary the discrimination threshold and calculate the TPR and FPR for each value. This is called an ROC curve, which stands for reciever operator curve, and it allows us to understand a classification model's performance as the discrimination threshold is varied.To calculate the TPR and FPR values at each discrimination threshold, we can use the scikit-learn roc_curve function.\n",
    "\n",
    "This function takes 2 required parameters:\n",
    "- y_true: list of the true labels for the observations,\n",
    "- y_score: list of the model's probability scores for those observations.\n",
    "\n",
    " the roc_curve function returns 3 values which you can assign all at once:\n",
    "- fpr, tpr, thresholds = metrics.roc_curve(labels, probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEACAYAAABWLgY0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmCEvSQ0z5CWpYf8PBSUQgWxg7G4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6c175b7950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn import metrics\n",
    "\n",
    "probabilities = logical_model.predict_proba(test[[\"gpa\"]])\n",
    "fpr,tpr,thresholds = metrics.roc_curve(test[\"actual_label\"],probabilities[:,1])\n",
    "plt.plot(fpr,tpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Area under the curve\n",
    "When looking at an ROC curve, you want to keep an eye on how the 2 measures trade off and select an appropriate threshold based on your priorities.\n",
    "\n",
    "We can now go one step further and determine the area under the curve or AUC for short.Since the True Positive Rate and the False Positive Rate add upto a 1.0, the AUC will be less than 1.0 as well. The AUC describes the probability that the classifier will rank a random positive observation higher than a random negative observation. Since randomly guessing converges to a probability of 0.5, the higher the AUC the more accurate the model seems to be.\n",
    "\n",
    "To calculate the AUC, we can use the scikit-learn function roc_auc_score, which takes the same parameters as the roc_curve function and returns a single float value corresponding to the AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.61602933473\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "auc_score = roc_auc_score(test[\"actual_label\"],probabilities[:,1])\n",
    "print(auc_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
