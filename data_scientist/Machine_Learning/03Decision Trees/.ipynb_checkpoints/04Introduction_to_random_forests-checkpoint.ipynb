{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Introduction\n",
    "The most powerful method to reduce decision tree overfitting is called the random forest algorithm. In this mission, we'll learn how to construct and apply random forests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "names = ['age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
    "           'marital_status', 'occupation', 'relationship', 'race', 'sex',\n",
    "           'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'high_income']\n",
    "income = pd.read_csv('adult.data',names=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_col = ['workclass','education', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'native_country', 'high_income']\n",
    "\n",
    "for target in target_col:\n",
    "    col = pd.Categorical.from_array(income[target])\n",
    "    income[target] = col.codes\n",
    "\n",
    "columns = [\"age\", \"workclass\", \"education_num\", \"marital_status\",\n",
    "           \"occupation\", \"relationship\", \"race\", \"sex\", \"hours_per_week\", \"native_country\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "income = income.reindex(np.random.permutation(income.index))\n",
    "\n",
    "train_max_row = int(math.floor(income.shape[0]*.8))\n",
    "\n",
    "train = income.iloc[:train_max_row]\n",
    "test= income.iloc[train_max_row:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Ensemble models\n",
    "A random forest is a kind of ensemble model. Ensembles combine the predictions of multiple models to create a more accurate final prediction. We'll make a simple ensemble to see how it works.\n",
    "\n",
    "We'll create two decision trees with slightly different parameters, and check their accuracy separately. Later on, we'll combine their predictions and compare the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.784853009097 0.771031199892\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=1,min_samples_leaf=75)\n",
    "clf.fit(train[columns],train['high_income'])\n",
    "\n",
    "clf2 = DecisionTreeClassifier(random_state=1,max_depth=6)\n",
    "clf2.fit(train[columns],train['high_income'])\n",
    "\n",
    "predictions_clf1 = clf.predict(test[columns])\n",
    "predictions_clf2 = clf2.predict(test[columns])\n",
    "\n",
    "auc_clf1 = roc_auc_score(predictions_clf1,test['high_income'])\n",
    "auc_clf2 = roc_auc_score(predictions_clf2,test['high_income'])\n",
    "\n",
    "print auc_clf1,auc_clf2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Combining our predictions\n",
    "When we have multiple classifiers making predictions, we can treat each set of predictions as a column in a matrix.\n",
    "\n",
    "Ultimately, we don't want this matrix, though -- we want one prediction per row in the training data. To do this, we'll need to create rules to turn each row of our matrix of predictions into a single number.\n",
    "\n",
    "There are many ways to get from the output of multiple models to a final vector of predictions. One method is majority voting, where each classifier gets a \"vote\", and the most commonly voted value for each row wins. \n",
    "\n",
    "This only works if there are more than 2 classifiers.\n",
    "\n",
    "we'll have to use a different method to combine predictions.\n",
    "\n",
    "We'll take the mean of all the items in a row. Right now, we're using the predict method, which returns either 0 or 1.\n",
    "\n",
    "We can instead use the predict_proba method, which will predict a probability from 0 to 1 that a given class is the right one for a row. Since 0 and 1 are our two classes, we'll get a matrix with as many rows as the income dataframe and 2 columns.\n",
    "\n",
    "If we just take the second column, we get the average value that the classifier would predict for that row. If there's a .9 probability that the correct classification is 1, we can use the .9 as the value the classifier is predicting. This will give us a continuous output in a single vector instead of just 0 or 1. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.789959895266\n"
     ]
    }
   ],
   "source": [
    "predictions = clf.predict_proba(test[columns])[:,1]\n",
    "predictions2 = clf2.predict_proba(test[columns])[:,1]\n",
    "\n",
    "averages = (predictions + predictions2) / 2.\n",
    "averages_round = np.round(averages)\n",
    "\n",
    "print(roc_auc_score(averages_round,test['high_income']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4: Why ensembling works\n",
    "As we can see from the previous screen, the combined predictions of the two trees are more accurate than any single tree.\n",
    "\n",
    "Both models are approaching the problem slightly differently, and building a different tree because we used different parameters for each. Each tree makes different predictions in different areas. Even though both trees have about the same accuracy, when we combine them, the result is stronger because it leverages the strengths of both approaches.\n",
    "\n",
    "The more \"diverse\", or dissimilar, the models used to construct an ensemble, the stronger the combined predictions will be. Ensembling a decision tree and a logistic regression model, which use very different approaches to arrive at their answers, will result in stronger predictions than ensembling two decision trees with similar parameters.\n",
    "\n",
    "On the other side, if the models you ensemble are very similar in how they make predictions, you'll get a negligible boost from ensembling.\n",
    "\n",
    "Ensembling models with very different accuracies will not generally improve your accuracy. Ensembling a model with a .75 AUC and a model with a .85 AUC on a test set will usually result in an AUC somewhere in between the two original values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 5: Bagging\n",
    "In order to make ensembling effective, we have to introduce variation into each individual decision tree model.\n",
    "\n",
    "There are two main ways to introduce variation in a random forest -- bagging and random feature subsets. We'll dive into bagging first.\n",
    "\n",
    "In a random forest, each tree isn't trained using the whole dataset. Instead, it's trained on a random sample of the data, or a \"bag\".This sampling is performed with replacement. When we sample with replacement, after we select a row from the data we're sampling, we put the row back in the data so it can be picked again. Some rows from the original data may appear in the \"bag\" multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.785415640465\n"
     ]
    }
   ],
   "source": [
    "# We'll build 10 trees\n",
    "tree_count = 10\n",
    "\n",
    "#Each bag will have 60% of the number of original rows\n",
    "bag_proportion = .6\n",
    "\n",
    "predictions = []\n",
    "for i in range(tree_count):\n",
    "    # We select 60% of the rows from train, sampling with replacement.\n",
    "    # We set a random state to ensure we'll be able to replicate our results.\n",
    "    # We set it to i instead of a fixed values so we don't get the same sample every loop.\n",
    "    # That would make all of our trees the same.\n",
    "    bag = train.sample(frac=bag_proportion,replace=True,random_state=i)\n",
    "    \n",
    "    clf = DecisionTreeClassifier(random_state=1,min_samples_leaf=75)\n",
    "    clf.fit(bag[columns],bag['high_income'])\n",
    "    \n",
    "    #Using the model, make predictions on the test data.\n",
    "    predictions.append(clf.predict_proba(test[columns])[:,1])\n",
    "predictions_average = np.mean(predictions,axis=0)\n",
    "predictions_average_round = np.round(predictions_average)\n",
    "\n",
    "print(roc_auc_score(predictions_average_round,test['high_income']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6: Selecting random features\n",
    "With the bagging example from the previous screen, we gained some accuracy over a single decision tree. Now, we'll go back to the decision tree algorithm we created 2 missions ago to explain random feature subsets. \n",
    "\n",
    "We first pick a maximum number of features that we want to evaluate each time we split the tree. This has to be less than the total number of columns in the data.\n",
    "\n",
    "Every time we split, we pick a random sample of features from the data. We then compute the information gain for each feature in our random sample, and pick the one with the highest information gain to split on.\n",
    "\n",
    "We're repeating the same process to select the optimal split for a node, but we'll only evaluate a constrained set of features, selected randomly. This introduces variation into the trees, and makes for more powerful ensembles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calc_entropy(column):\n",
    "    # Compute the counts of each unique value in the column\n",
    "    counts = np.bincount(column)\n",
    "    probabilities = counts/float(len(column))\n",
    "    \n",
    "    entropy = 0\n",
    "    \n",
    "    for prob in probabilities:\n",
    "        if prob > 0:\n",
    "            entropy += prob*math.log(prob,2)\n",
    "            \n",
    "    return -entropy\n",
    "\n",
    "def calc_information_gain(data,split_name,target_name):\n",
    "    #Calculate original entropy.\n",
    "    original_entropy = calc_entropy(data[target_name])\n",
    "    \n",
    "    #Find the median of the column we're spliting\n",
    "    column = data[split_name]\n",
    "    median = column.median()\n",
    "    \n",
    "    left_split = data[column <= median]\n",
    "    right_split = data[column > median]\n",
    "    \n",
    "    #Loop through the splits, and calculate the subset entropy\n",
    "    to_subtract = 0\n",
    "    for subset in [left_split,right_split]:\n",
    "        prob = (subset.shape[0] / float(data.shape[0]))\n",
    "        to_subtract += prob * calc_entropy(subset[target_name])\n",
    "    \n",
    "    #return information gain\n",
    "    return original_entropy - to_subtract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'column': 'age', 'right': {'column': 'age', 'right': {'number': 11, 'label': 0}, 'median': 55.0, 'number': 7, 'left': {'column': 'age', 'right': {'number': 10, 'label': 1}, 'median': 47.5, 'number': 8, 'left': {'number': 9, 'label': 0}}}, 'median': 37.5, 'number': 1, 'left': {'column': 'employment', 'right': {'number': 6, 'label': 1}, 'median': 4.0, 'number': 2, 'left': {'column': 'age', 'right': {'number': 5, 'label': 1}, 'median': 22.5, 'number': 3, 'left': {'number': 4, 'label': 0}}}}\n"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame([\n",
    "    [0,4,20,0],\n",
    "    [0,4,60,2],\n",
    "    [0,5,40,1],\n",
    "    [1,4,25,1],\n",
    "    [1,5,35,2],\n",
    "    [1,5,55,1]\n",
    "    ])\n",
    "\n",
    "data.columns = [\"high_income\", \"employment\", \"age\", \"marital_status\"]\n",
    "\n",
    "# Set a random seed to make results reproducible.\n",
    "np.random.seed(1)\n",
    "\n",
    "tree = {}\n",
    "nodes = []\n",
    "\n",
    "# The function to find the column to split on.\n",
    "def find_best_column(data,target_name,columns):\n",
    "    information_gains = []\n",
    "    \n",
    "    cols = np.random.choice(columns,2)\n",
    "    \n",
    "    for col in cols:\n",
    "        information_gain = calc_information_gain(data,col,'high_income')\n",
    "        information_gains.append(information_gain)\n",
    "        \n",
    "    highest_gain_index = information_gains.index(max(information_gains))\n",
    "    \n",
    "    highest_gain = cols[highest_gain_index]\n",
    "    \n",
    "    return highest_gain\n",
    "\n",
    "# The function to construct an id3 decision tree.\n",
    "def id3(data, target, columns, tree):\n",
    "    unique_targets = pd.unique(data[target])\n",
    "    nodes.append(len(nodes) + 1)\n",
    "    tree[\"number\"] = nodes[-1]\n",
    "\n",
    "    if len(unique_targets) == 1:\n",
    "        if 0 in unique_targets:\n",
    "            tree[\"label\"] = 0\n",
    "        elif 1 in unique_targets:\n",
    "            tree[\"label\"] = 1\n",
    "        return\n",
    "    \n",
    "    best_column = find_best_column(data, target, columns)\n",
    "    column_median = data[best_column].median()\n",
    "    \n",
    "    tree[\"column\"] = best_column\n",
    "    tree[\"median\"] = column_median\n",
    "    \n",
    "    left_split = data[data[best_column] <= column_median]\n",
    "    right_split = data[data[best_column] > column_median]\n",
    "    split_dict = [[\"left\", left_split], [\"right\", right_split]]\n",
    "    \n",
    "    for name, split in split_dict:\n",
    "        tree[name] = {}\n",
    "        id3(split, target, columns, tree[name])\n",
    "\n",
    "id3(data,'high_income',['employment','age','martial_status'],tree)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7: Random subsets in scikit-learn\n",
    "We can also repeat our random subset selection process in scikit-learn. We just set the splitter parameter on DecisionTreeClassifier to \"random\", and the max_features parameter to \"auto\". If we have N columns, this will pick a subset of features of size N−−√, compute the gini coefficient (similar to information gain) for each, and split the node on the best column in the subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.789767997764\n"
     ]
    }
   ],
   "source": [
    "tree_count = 10\n",
    "\n",
    "bag_proportion = .7\n",
    "\n",
    "predictions = []\n",
    "for i in range(tree_count):\n",
    "    bag = train.sample(frac=bag_proportion,replace=True,random_state=i)\n",
    "    # Fit a decision tree model to the 'bag'\n",
    "    clf = DecisionTreeClassifier(random_state=1,min_samples_leaf=75,splitter='random',max_features='auto')\n",
    "    clf.fit(bag[columns],bag['high_income'])\n",
    "    \n",
    "    predictions.append(clf.predict_proba(test[columns])[:,1])\n",
    "    \n",
    "combined = np.sum(predictions,axis=0) / 10.\n",
    "rounded = np.round(combined)\n",
    "\n",
    "print(roc_auc_score(rounded,test['high_income']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8: Putting it all together\n",
    "When we instantiate a RandomForestClassifier, we pass in an n_estimators parameter that indicates how many trees to build. There's never any harm in building more trees, but each tree will take time to build, so more trees will take longer.\n",
    "\n",
    "RandomForestClassifier has a similar interface to DecisionTreeClassifier, and we can use the fit and predict methods to train and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.791634978035\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=10,random_state=1,min_samples_leaf=75)\n",
    "clf.fit(train[columns],train['high_income'])\n",
    "\n",
    "print(roc_auc_score(clf.predict(test[columns]),test['high_income']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9: Parameter tweaking\n",
    "Similarly to decision trees, we can tweak a few parameters with random forests:\n",
    "\n",
    "- min_samples_leaf\n",
    "- min_samples_split\n",
    "- max_depth\n",
    "- max_leaf_nodes\n",
    "\n",
    "These parameters apply to the individual trees in the model, and change how they are constructed. There are also parameters specific to the random forest that alter how it's constructed as a whole:\n",
    "\n",
    "- n_estimators\n",
    "- bootstrap -- defaults to True. Bootstrap aggregation is another name for bagging, and this indicates whether to turn it on.\n",
    "\n",
    "By tweaking parameters, we can increase the accuracy of the forest. The easiest tweak is to increase the number of estimators we use. This has diminishing returns -- going from 10 trees to 100 will make a bigger difference than going from 100 to 500, which will make a bigger difference than going from 500 to 1000. The accuracy increase function is logarithmic, so increasing the number of trees beyond a certain number (usually 200) won't help much at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.793788646293\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=150,random_state=1,min_samples_leaf=75)\n",
    "\n",
    "clf.fit(train[columns],train['high_income'])\n",
    "\n",
    "predictions = clf.predict(test[columns])\n",
    "print(roc_auc_score(predictions,test['high_income']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10: Reducing overfitting\n",
    "One of the major advantages of random forests over single decision trees is they overfit less. Although each individual decision tree in a random forest varies widely, the average of their predictions is less sensitive to the input data than a single tree is. This is because while one tree can construct an incorrect and overfit model, the average of 100 or more trees will be more likely to hone in on the signal and ignore the noise. The signal will be the same across all the trees, whereas each tree will hone into the noise differently. This means that the average will discard the noise and keep the signal.\n",
    "\n",
    "In the code cell, you'll see that we've fit a single decision tree to the training set, and made predictions for both the training set and testing set. The AUC for the training set predictions is .789, and the AUC for the testing set is .784. This indicates a mild degree of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.789854859593\n",
      "0.784853009097\n",
      "0.794137608735\n",
      "0.793788646293\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(random_state=1,min_samples_leaf=75)\n",
    "clf.fit(train[columns],train['high_income'])\n",
    "\n",
    "predictions = clf.predict(train[columns])\n",
    "print roc_auc_score(predictions,train['high_income'])\n",
    "\n",
    "predictions = clf.predict(test[columns])\n",
    "print roc_auc_score(predictions,test['high_income'])\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=150,random_state=1,min_samples_leaf=75)\n",
    "clf.fit(train[columns],train['high_income'])\n",
    "\n",
    "predictions = clf.predict(train[columns])\n",
    "print roc_auc_score(predictions,train['high_income'])\n",
    "\n",
    "predictions = clf.predict(test[columns])\n",
    "print roc_auc_score(predictions,test['high_income'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11: When to use random forests\n",
    "The random forest algorithm is incredibly powerful, but isn't applicable to all tasks. The main strengths of a random forest are:\n",
    "- Very accurate predictions -- Random forests achieve near state of the art performance on many machine learning tasks. Along with neural networks and gradient boosted trees, they are typically one of the top performing algorithms.\n",
    "- Resistance to overfitting -- due to how they're constructed, random forests are fairly resistant to overfitting. Parameters like max_depth still have to be set and tweaked, though.\n",
    "\n",
    "The main weaknesses are:\n",
    "- Hard to interpret -- because we've averaging the results of many trees, it can be hard to figure out why a random forest is making predictions the way it is.\n",
    "\n",
    "- Longer creation time -- making two trees takes twice as long as making one, 3 takes three times as long, and so on. Luckily, we can exploit multicore processors to parallelize tree construction. Scikit allows us to do this through the n_jobs parameter on RandomForestClassifier. We'll get more into parallelization later.\n",
    "\n",
    "Given these tradeoffs, it makes sense to use random forests in situations where accuracy is of the utmost importance, and being able to interpret or explain the decisions the model is making isn't key. In cases where time is of the essence, or interpretability is important, a single decision tree may be a better choice.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
