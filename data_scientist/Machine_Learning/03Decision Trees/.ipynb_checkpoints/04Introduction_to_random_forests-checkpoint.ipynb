{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Introduction\n",
    "The most powerful method to reduce decision tree overfitting is called the random forest algorithm. In this mission, we'll learn how to construct and apply random forests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "names = ['age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
    "           'marital_status', 'occupation', 'relationship', 'race', 'sex',\n",
    "           'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'high_income']\n",
    "income = pd.read_csv('adult.data',names=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_col = ['workclass','education', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'native_country', 'high_income']\n",
    "\n",
    "for target in target_col:\n",
    "    col = pd.Categorical.from_array(income[target])\n",
    "    income[target] = col.codes\n",
    "\n",
    "columns = [\"age\", \"workclass\", \"education_num\", \"marital_status\",\n",
    "           \"occupation\", \"relationship\", \"race\", \"sex\", \"hours_per_week\", \"native_country\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "income = income.reindex(np.random.permutation(income.index))\n",
    "\n",
    "train_max_row = int(math.floor(income.shape[0]*.8))\n",
    "\n",
    "train = income.iloc[:train_max_row]\n",
    "test= income.iloc[train_max_row:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Ensemble models\n",
    "A random forest is a kind of ensemble model. Ensembles combine the predictions of multiple models to create a more accurate final prediction. We'll make a simple ensemble to see how it works.\n",
    "\n",
    "We'll create two decision trees with slightly different parameters, and check their accuracy separately. Later on, we'll combine their predictions and compare the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.784853009097 0.771031199892\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=1,min_samples_leaf=75)\n",
    "clf.fit(train[columns],train['high_income'])\n",
    "\n",
    "clf2 = DecisionTreeClassifier(random_state=1,max_depth=6)\n",
    "clf2.fit(train[columns],train['high_income'])\n",
    "\n",
    "predictions_clf1 = clf.predict(test[columns])\n",
    "predictions_clf2 = clf2.predict(test[columns])\n",
    "\n",
    "auc_clf1 = roc_auc_score(predictions_clf1,test['high_income'])\n",
    "auc_clf2 = roc_auc_score(predictions_clf2,test['high_income'])\n",
    "\n",
    "print auc_clf1,auc_clf2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Combining our predictions\n",
    "When we have multiple classifiers making predictions, we can treat each set of predictions as a column in a matrix.\n",
    "\n",
    "Ultimately, we don't want this matrix, though -- we want one prediction per row in the training data. To do this, we'll need to create rules to turn each row of our matrix of predictions into a single number.\n",
    "\n",
    "There are many ways to get from the output of multiple models to a final vector of predictions. One method is majority voting, where each classifier gets a \"vote\", and the most commonly voted value for each row wins. \n",
    "\n",
    "This only works if there are more than 2 classifiers.\n",
    "\n",
    "we'll have to use a different method to combine predictions.\n",
    "\n",
    "We'll take the mean of all the items in a row. Right now, we're using the predict method, which returns either 0 or 1.\n",
    "\n",
    "We can instead use the predict_proba method, which will predict a probability from 0 to 1 that a given class is the right one for a row. Since 0 and 1 are our two classes, we'll get a matrix with as many rows as the income dataframe and 2 columns.\n",
    "\n",
    "If we just take the second column, we get the average value that the classifier would predict for that row. If there's a .9 probability that the correct classification is 1, we can use the .9 as the value the classifier is predicting. This will give us a continuous output in a single vector instead of just 0 or 1. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.789959895266\n"
     ]
    }
   ],
   "source": [
    "predictions = clf.predict_proba(test[columns])[:,1]\n",
    "predictions2 = clf2.predict_proba(test[columns])[:,1]\n",
    "\n",
    "averages = (predictions + predictions2) / 2.\n",
    "averages_round = np.round(averages)\n",
    "\n",
    "print(roc_auc_score(averages_round,test['high_income']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4: Why ensembling works\n",
    "As we can see from the previous screen, the combined predictions of the two trees are more accurate than any single tree.\n",
    "\n",
    "Both models are approaching the problem slightly differently, and building a different tree because we used different parameters for each. Each tree makes different predictions in different areas. Even though both trees have about the same accuracy, when we combine them, the result is stronger because it leverages the strengths of both approaches.\n",
    "\n",
    "The more \"diverse\", or dissimilar, the models used to construct an ensemble, the stronger the combined predictions will be. Ensembling a decision tree and a logistic regression model, which use very different approaches to arrive at their answers, will result in stronger predictions than ensembling two decision trees with similar parameters.\n",
    "\n",
    "On the other side, if the models you ensemble are very similar in how they make predictions, you'll get a negligible boost from ensembling.\n",
    "\n",
    "Ensembling models with very different accuracies will not generally improve your accuracy. Ensembling a model with a .75 AUC and a model with a .85 AUC on a test set will usually result in an AUC somewhere in between the two original values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
