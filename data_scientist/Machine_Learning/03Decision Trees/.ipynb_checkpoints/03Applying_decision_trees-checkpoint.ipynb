{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used a modified version of ID3, which is a bit simpler than the most common tree building algorithms, C4.5, and CART. However, the basics are all the same, and so we can apply the principles we learned about how decision trees work to any tree construction algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Using decision trees with scikit-learn\n",
    "We can use the scikit-learn package to fit a decision tree. The interface is very similar to other algorithms we've fit in the past.\n",
    "\n",
    "We use the DecisionTreeClassifier class for classification problems, and DecisionTreeRegressor for regression problems. Both of these classes are in the sklearn.tree package.\n",
    "\n",
    "In this case, we're predicting a binary outcome, so we'll use a classifier.\n",
    "\n",
    "The first step is to train the classifier on the data. We'll use the fit method on a classifier to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "names = ['age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
    "           'marital_status', 'occupation', 'relationship', 'race', 'sex',\n",
    "           'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'high_income']\n",
    "income = pd.read_csv('adult.data',names=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=1, splitter='best')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_col = ['workclass','education', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'native_country', 'high_income']\n",
    "\n",
    "for target in target_col:\n",
    "    col = pd.Categorical.from_array(income[target])\n",
    "    income[target] = col.codes\n",
    "\n",
    "columns = [\"age\", \"workclass\", \"education_num\", \"marital_status\",\n",
    "           \"occupation\", \"relationship\", \"race\", \"sex\", \"hours_per_week\", \"native_country\"]\n",
    "# Instantiate the classifier\n",
    "# Set random_state to 1 to keep results consistent\n",
    "clf = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "clf.fit(income[columns],income['high_income'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Splitting the data into train and test sets\n",
    "Now that we've fit a model, we can make predictions. We'll want to split our data into training and testing sets first.\n",
    "\n",
    "We can avoid overfitting by always making predictions and evaluating error on data that our algorithm hasn't been trained with. This will show us when we're overfitting by giving us a realistic error on data that the algorithm hasn't seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "income = income.reindex(np.random.permutation(income.index))\n",
    "\n",
    "train_max_row = int(math.floor(income.shape[0]*.8))\n",
    "\n",
    "train = income.iloc[:train_max_row]\n",
    "test= income.iloc[train_max_row:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4: Evaluating error\n",
    " AUC ranges from 0 to 1, and is ideal for binary classification. The higher the AUC, the more accurate our predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.701110628846\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=1)\n",
    "clf.fit(train[columns], train[\"high_income\"])\n",
    "\n",
    "predictions = clf.predict(test[columns])\n",
    "error = roc_auc_score(predictions,test['high_income'])\n",
    "\n",
    "print error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5: Compute error on the training set\n",
    "The AUC for the predictions on the testing set is about .7. Let's compare this against the AUC for predictions on the training set to see if the model is overfitting. \n",
    "\n",
    "It's normal for the model to predict the training set better than the testing set. After all, it has full knowledge of that data and the outcomes. However, if the AUC between training set predictions and actual values is significantly higher than the AUC between test set predictions and actual values, it's a sign that the model may be overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97449683987\n"
     ]
    }
   ],
   "source": [
    "predictions = clf.predict(train[columns])\n",
    "print(roc_auc_score(predictions,train['high_income']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6: Decision tree overfitting\n",
    " There's no hard and fast rule on when overfitting is happening, but our model is predicting the training set much better than it's predicting the test set. Splitting the data into training and testing sets doesn't prevent overfitting -- it just helps us detect it and fix it.\n",
    "\n",
    "We've built our tree in such a way that it can perfectly predict the training set -- but, the way the tree has been constructed doesn't make sense when we step back.\n",
    "\n",
    "These rules are very specific to the training set.\n",
    "\n",
    "All we've done is \"pruned\" the tree, and removed some of the lower leaves. We've made some of the higher up nodes into leaves instead.\n",
    "\n",
    "This actually has lower accuracy on our training set, but it will generalize better to new examples, because it matches reality better.\n",
    "\n",
    "Trees overfit when they have too much depth, and make overly complex rules that match the training data, but aren't able to generalize well to new data.\n",
    "\n",
    "This may seem to be a strange principle at first, but the more depth a tree has, typically the worse it performs on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7: Building a shallower tree\n",
    "There are three main ways to combat overfitting:\n",
    "\n",
    "- \"Prune\" the tree after building to remove unneeded leaves.\n",
    "-  Use ensembling to blend the predictions of many trees.\n",
    "- Restrict the depth of the tree while you're building it.\n",
    "\n",
    "We can restrict how deep the tree is built with a few parameters when we initialize the DecisionTreeClassifier class:\n",
    "\n",
    "- max_depth -- this globally restricts how deep the tree can go.\n",
    "- min_samples_split -- The minimum number of rows needed in a node before it can be split. For example, if this is set to 2, then nodes with 2 rows won't be split, and will become leaves instead.\n",
    "- min_samples_leaf -- the minimum number of rows that a leaf must have.\n",
    "- min_weight_fraction_leaf -- the fraction of input rows that are required to be at a leaf.\n",
    "- max_leaf_nodes -- the maximum number of total leaves. This will cap the count of leaf nodes as the tree is being built.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.933086058571 0.719213185068\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(random_state=1,min_samples_split=5)\n",
    "clf.fit(train[columns],train['high_income'])\n",
    "\n",
    "predictions_train = clf.predict(train[columns])\n",
    "train_auc = roc_auc_score(predictions_train,train['high_income'])\n",
    "\n",
    "predictions_test = clf.predict(test[columns])\n",
    "test_auc = roc_auc_score(predictions_test,test['high_income'])\n",
    "\n",
    "print train_auc,test_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
