{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Decision Trees\n",
    "Decision trees are a powerful and commonly used machine learning technique.\n",
    "\n",
    "If we wanted to optimize our chances of surviving a bear encounter, we could construct a decision tree to tell us what action to take.As our dataset gets larger though, this gets less and less practical. What if we had 10000 rows and 10 variables? Would you want to look through all the possibilities to construct a tree?\n",
    "\n",
    "This is where the decision tree machine learning algorithm can help. It enables us to automatically construct a decision tree to tell us what outcomes we should predict in certain situations.\n",
    "\n",
    "The decision tree algorithm is a supervised learning algorithm -- we first construct the tree with historical data, and then use it to predict an outcome. One of the major advantages of decision trees is that they can pick up nonlinear interactions between variables in the data that linear regression can't.\n",
    "\n",
    "Trees can be used for classification or regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "columns = ['age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
    "           'marital_status', 'occupation', 'relationship', 'race', 'sex',\n",
    "           'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'high_income']\n",
    "income = pd.read_csv('adult.data',names=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age          workclass  fnlwgt   education  education_num  \\\n",
      "0   39          State-gov   77516   Bachelors             13   \n",
      "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
      "2   38            Private  215646     HS-grad              9   \n",
      "3   53            Private  234721        11th              7   \n",
      "4   28            Private  338409   Bachelors             13   \n",
      "\n",
      "        marital_status          occupation    relationship    race      sex  \\\n",
      "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
      "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
      "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
      "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
      "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
      "\n",
      "   capital_gain  capital_loss  hours_per_week  native_country high_income  \n",
      "0          2174             0              40   United-States       <=50K  \n",
      "1             0             0              13   United-States       <=50K  \n",
      "2             0             0              40   United-States       <=50K  \n",
      "3             0             0              40   United-States       <=50K  \n",
      "4             0             0              40            Cuba       <=50K  \n"
     ]
    }
   ],
   "source": [
    "print income.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Converting categorical variables\n",
    "we need to convert the categorical variables in our dataset to numeric variables. This involves assigning a number to each category label, and converting all the labels in a column to numbers.\n",
    "\n",
    "We can use the Categorical.from_array method from Pandas to perform the conversion to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    7\n",
      "1    6\n",
      "2    4\n",
      "3    4\n",
      "4    4\n",
      "Name: workclass, dtype: int8\n"
     ]
    }
   ],
   "source": [
    "col = pd.Categorical.from_array(income['workclass'])\n",
    "income['workclass'] = col.codes\n",
    "print(income['workclass'].head())\n",
    "\n",
    "target_col = ['education', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'native_country', 'high_income']\n",
    "\n",
    "for target in target_col:\n",
    "    col = pd.Categorical.from_array(income[target])\n",
    "    income[target] = col.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4: Split points\n",
    "A decision tree is constructed through a series of nodes and branches. A node is where we split the data based on a variable, and a branch is one side of the split. Here's an example:å›³\n",
    "\n",
    "This is exactly how a decision tree works -- we keep splitting the data based on variables. As we do this, the tree gets deeper and deeper. The tree we made above is two levels deep, because it has one split, and two \"levels\" of nodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5: Performing a split\n",
    "In the diagram above, we can split the dataset into two portions based on whether the individual works in the private sector or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "private_incomes = income[income['workclass'] == 4]\n",
    "public_incomes = income[income['workclass'] != 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6: Thinking about the data\n",
    "When we make a split, some rows will go to the right, and some rows will go to the left. As we build the tree deeper and deeper, each node will \"receive\" less and less rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7: Rationale behind splitting\n",
    "The nodes at the bottom of the tree, when we decide to stop splitting, are called terminal nodes, or leaves.Our goal is to ensure that we can make a prediction on future data. In order to do this, all rows in each leaf must have only one value for our target column.\n",
    "\n",
    "In order to be able to make this prediction, all leaves need to have rows with only a single value for high_income."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8: Entropy\n",
    "we need a metric for how \"together\" the different values in the high_income column are.A dataset consisting of only 1s in the high_income column would have low entropy.\n",
    "\n",
    "\n",
    "Entropy, which is not to be confused with entropy from physics, comes from information theory.Entropy can also be thought of in terms of information. If we flip a coin where both sides are heads, we know upfront that the result will be heads. There's much more complexity, especially when you get to cases with more than two possible outcomes, or differential probabilities.\n",
    "\n",
    "Entropy can also be thought of in terms of information.\n",
    "\n",
    "$$-\\sum_{i=1}^{c} {\\mathrm{P}(x_i) \\log_b \\mathrm{P}(x_i)}$$\n",
    "\n",
    "We iterate through each unique value in a single column (in this case, high_income), and assign it to i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.796383955202\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "p1 = income[income['high_income']==1].shape[0] / float(income.shape[0])\n",
    "p2 = income[income['high_income']==0].shape[0] / float(income.shape[0])\n",
    "income_entropy = -(p1*math.log(p1,2) + p2*math.log(p2,2))\n",
    "print income_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9: Information gain\n",
    "We'll need a way to go from computing entropy to figuring out which variable to split based on. We can do this using information gain. Information gain tells us which split will reduce entropy the most.\n",
    "\n",
    "Here's the formula for information gain:\n",
    "$$IG(T,A) = Entropy(T)-\\sum_{v\\in A}\\frac{|T_{v}|}{|T|} \\cdot Entropy(T_{v})$$\n",
    "\n",
    "We're computing information gain (IG) for a given target variable (T), and a given variable that we want to split based on (A).  To compute this, we first calculate the entropy for T. Then, for each unique value v in the variable A, we compute the number of rows in which A takes on the value v, and divide it by the total number of rows. We then multiply this by the entropy of the rows where A is v. We add together all of these subset entropies, and subtract from the overall entropy to get information gain.\n",
    "\n",
    "For an even simpler explanation, we're finding the entropy of each set post-split, weighting it by the number of items in each split, then subtracting from the current entropy. If it's positive, we've lowered entropy with our split. The higher it is, the more we've lowered entropy.\n",
    "\n",
    "One strategy to construct trees to is to create as many branches at each node as there are unique values for the variable being split on. So if the variable has 3 or 4 values, that would result in 3 or 4 branches. Usually, this is more complexity than it's worth, and doesn't improve prediction accuracy, but it's worth knowing about.\n",
    "\n",
    "To simplify the calculation of information gain, and to make splits simpler, we won't do it for each unique value. Instead, for the variable we're splitting on, we'll find the median. Any rows where the value of the variable is below the median will split left, and the rest of the rows will split right. To compute information gain, we'll only have to compute entropies for two subsets.ä¾‹ç¤º\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.970950594455\n",
      "0.170950594455\n",
      "0.0470286613047\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calc_entropy(column):\n",
    "    # Compute the counts of each unique value in the column\n",
    "    counts = np.bincount(column)\n",
    "    probabilities = counts/float(len(column))\n",
    "    \n",
    "    entropy = 0\n",
    "    \n",
    "    for prob in probabilities:\n",
    "        if prob > 0:\n",
    "            entropy += prob*math.log(prob,2)\n",
    "            \n",
    "    return -entropy\n",
    "\n",
    "entropy = calc_entropy([1,1,0,0,1])\n",
    "print entropy\n",
    "\n",
    "information_gain = entropy - ((.8 * calc_entropy([1,1,0,0])) + (.2 * calc_entropy([1])))\n",
    "print information_gain\n",
    "\n",
    "income_entropy = calc_entropy(income['high_income'])\n",
    "\n",
    "median = income['age'].median()\n",
    "left_incomes = income[income['age'] <= median]\n",
    "right_incomes = income[income['age'] > median]\n",
    "left_prob = left_incomes.shape[0] / float(income.shape[0])\n",
    "\n",
    "age_information_gain = income_entropy - ((left_prob*calc_entropy(left_incomes['high_income'])) + (1-left_prob)*calc_entropy(right_incomes['high_income']))\n",
    "print age_information_gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10: Finding the best split\n",
    "We'll find the variable to split on by finding which split would have the highest information gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0470286613047\n",
      "[0.047028661304691965, 0.0068109840543966182, 0.065012984132774232, 0.1114272573715438, 0.0015822303843424645, 0.047362416650269412, 0.0, 0.0, 0.040622468671234868, 0.00013457344495848567] marital_status\n"
     ]
    }
   ],
   "source": [
    "def calc_information_gain(data,split_name,target_name):\n",
    "    #Calculate original entropy.\n",
    "    original_entropy = calc_entropy(data[target_name])\n",
    "    \n",
    "    #Find the median of the column we're spliting\n",
    "    column = data[split_name]\n",
    "    median = column.median()\n",
    "    \n",
    "    left_split = data[column <= median]\n",
    "    right_split = data[column > median]\n",
    "    \n",
    "    #Loop through the splits, and calculate the subset entropy\n",
    "    to_subtract = 0\n",
    "    for subset in [left_split,right_split]:\n",
    "        prob = (subset.shape[0] / float(data.shape[0]))\n",
    "        to_subtract += prob * calc_entropy(subset[target_name])\n",
    "    \n",
    "    #return information gain\n",
    "    return original_entropy - to_subtract\n",
    "\n",
    "print calc_information_gain(income,'age','high_income')\n",
    "\n",
    "columns = [\"age\", \"workclass\", \"education_num\", \"marital_status\",\n",
    "           \"occupation\", \"relationship\", \"race\", \"sex\", \"hours_per_week\", \"native_country\"]\n",
    "\n",
    "information_gains = []\n",
    "\n",
    "for row in columns:\n",
    "    information_gain = calc_information_gain(income,row,'high_income')\n",
    "    information_gains.append(information_gain)\n",
    "\n",
    "highest_gain_index = information_gains.index(max(information_gains))\n",
    "highest_gain = columns[highest_gain_index]\n",
    "\n",
    "print information_gains,highest_gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11: Build the whole tree\n",
    "We now know how to make a single split. In order to construct a tree, we'll need to construct these splits all the way down the tree, until the leaves only have a single class.å›³\n",
    "\n",
    "In order to construct a tree like this, we'll need to be able to split each node multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
