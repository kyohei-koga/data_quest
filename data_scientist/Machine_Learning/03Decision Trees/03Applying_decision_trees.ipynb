{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used a modified version of ID3, which is a bit simpler than the most common tree building algorithms, C4.5, and CART. However, the basics are all the same, and so we can apply the principles we learned about how decision trees work to any tree construction algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Using decision trees with scikit-learn\n",
    "We can use the scikit-learn package to fit a decision tree. The interface is very similar to other algorithms we've fit in the past.\n",
    "\n",
    "We use the DecisionTreeClassifier class for classification problems, and DecisionTreeRegressor for regression problems. Both of these classes are in the sklearn.tree package.\n",
    "\n",
    "In this case, we're predicting a binary outcome, so we'll use a classifier.\n",
    "\n",
    "The first step is to train the classifier on the data. We'll use the fit method on a classifier to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "names = ['age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
    "           'marital_status', 'occupation', 'relationship', 'race', 'sex',\n",
    "           'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'high_income']\n",
    "income = pd.read_csv('adult.data',names=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=1, splitter='best')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_col = ['workclass','education', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'native_country', 'high_income']\n",
    "\n",
    "for target in target_col:\n",
    "    col = pd.Categorical.from_array(income[target])\n",
    "    income[target] = col.codes\n",
    "\n",
    "columns = [\"age\", \"workclass\", \"education_num\", \"marital_status\",\n",
    "           \"occupation\", \"relationship\", \"race\", \"sex\", \"hours_per_week\", \"native_country\"]\n",
    "# Instantiate the classifier\n",
    "# Set random_state to 1 to keep results consistent\n",
    "clf = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "clf.fit(income[columns],income['high_income'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Splitting the data into train and test sets\n",
    "Now that we've fit a model, we can make predictions. We'll want to split our data into training and testing sets first.\n",
    "\n",
    "We can avoid overfitting by always making predictions and evaluating error on data that our algorithm hasn't been trained with. This will show us when we're overfitting by giving us a realistic error on data that the algorithm hasn't seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "income = income.reindex(np.random.permutation(income.index))\n",
    "\n",
    "train_max_row = int(math.floor(income.shape[0]*.8))\n",
    "\n",
    "train = income.iloc[:train_max_row]\n",
    "test= income.iloc[train_max_row:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4: Evaluating error\n",
    " AUC ranges from 0 to 1, and is ideal for binary classification. The higher the AUC, the more accurate our predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.70319628094\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=1)\n",
    "clf.fit(train[columns], train[\"high_income\"])\n",
    "\n",
    "predictions = clf.predict(test[columns])\n",
    "error = roc_auc_score(predictions,test['high_income'])\n",
    "\n",
    "print error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5: Compute error on the training set\n",
    "The AUC for the predictions on the testing set is about .7. Let's compare this against the AUC for predictions on the training set to see if the model is overfitting. \n",
    "\n",
    "It's normal for the model to predict the training set better than the testing set. After all, it has full knowledge of that data and the outcomes. However, if the AUC between training set predictions and actual values is significantly higher than the AUC between test set predictions and actual values, it's a sign that the model may be overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.973887981629\n"
     ]
    }
   ],
   "source": [
    "predictions = clf.predict(train[columns])\n",
    "print(roc_auc_score(predictions,train['high_income']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6: Decision tree overfitting\n",
    " There's no hard and fast rule on when overfitting is happening, but our model is predicting the training set much better than it's predicting the test set. Splitting the data into training and testing sets doesn't prevent overfitting -- it just helps us detect it and fix it.\n",
    "\n",
    "We've built our tree in such a way that it can perfectly predict the training set -- but, the way the tree has been constructed doesn't make sense when we step back.\n",
    "\n",
    "These rules are very specific to the training set.\n",
    "\n",
    "All we've done is \"pruned\" the tree, and removed some of the lower leaves. We've made some of the higher up nodes into leaves instead.\n",
    "\n",
    "This actually has lower accuracy on our training set, but it will generalize better to new examples, because it matches reality better.\n",
    "\n",
    "Trees overfit when they have too much depth, and make overly complex rules that match the training data, but aren't able to generalize well to new data.\n",
    "\n",
    "This may seem to be a strange principle at first, but the more depth a tree has, typically the worse it performs on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7: Building a shallower tree\n",
    "There are three main ways to combat overfitting:\n",
    "\n",
    "- \"Prune\" the tree after building to remove unneeded leaves.\n",
    "-  Use ensembling to blend the predictions of many trees.\n",
    "- Restrict the depth of the tree while you're building it.\n",
    "\n",
    "We can restrict how deep the tree is built with a few parameters when we initialize the DecisionTreeClassifier class:\n",
    "\n",
    "- max_depth -- this globally restricts how deep the tree can go.\n",
    "- min_samples_split -- The minimum number of rows needed in a node before it can be split. For example, if this is set to 2, then nodes with 2 rows won't be split, and will become leaves instead.\n",
    "- min_samples_leaf -- the minimum number of rows that a leaf must have.\n",
    "- min_weight_fraction_leaf -- the fraction of input rows that are required to be at a leaf.\n",
    "- max_leaf_nodes -- the maximum number of total leaves. This will cap the count of leaf nodes as the tree is being built.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.934501028744 0.713198095219\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(random_state=1,min_samples_split=5)\n",
    "clf.fit(train[columns],train['high_income'])\n",
    "\n",
    "predictions_train = clf.predict(train[columns])\n",
    "train_auc = roc_auc_score(predictions_train,train['high_income'])\n",
    "\n",
    "predictions_test = clf.predict(test[columns])\n",
    "test_auc = roc_auc_score(predictions_test,test['high_income'])\n",
    "\n",
    "print train_auc,test_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 8: More parameter tweaking\n",
    "By restricting min_samples_split to 5, we managed to boost test AUC to .713 from .702. Training set AUC decreased from .971 to .934, showing that the model we built was less overfit to the training set than before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.796980156519 0.790555914131\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(random_state=1,max_depth=4,min_samples_split=25)\n",
    "clf.fit(train[columns],train['high_income'])\n",
    "predictions = clf.predict(test[columns])\n",
    "\n",
    "test_auc = roc_auc_score(predictions,test['high_income'])\n",
    "\n",
    "train_predictions = clf.predict(train[columns])\n",
    "train_auc = roc_auc_score(train_predictions,train['high_income'])\n",
    "\n",
    "print test_auc,train_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9: Tweaking the depth\n",
    "We just improved AUC again. The test set AUC is .796, and the training set AUC is .790. We aren't overfitting anymore, as both AUCs are about the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.774502262504 0.775022084636\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(random_state=1,min_samples_split=100,max_depth=2)\n",
    "clf.fit(train[columns],train['high_income'])\n",
    "predictions = clf.predict(test[columns])\n",
    "test_auc = roc_auc_score(predictions,test['high_income'])\n",
    "\n",
    "train_predictions = clf.predict(train[columns])\n",
    "train_auc = roc_auc_score(train_predictions,train['high_income'])\n",
    "\n",
    "print test_auc,train_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10: Underfitting\n",
    "Our accuracy went down in the past screen relative to the screen before. This is because we're now underfitting. Underfitting is what happens when our model is too simple to actually explain the relations between the variables.\n",
    "\n",
    "And here's the \"right fit\" tree. This tree explains the data properly, without overfitting:図\n",
    "\n",
    "Let's trim this tree even more to show what happens when the model isn't complex enough to explain the data:図\n",
    "\n",
    " This model is too simple to model reality -- which is younger people make less, middle-aged people make more, and elderly people make less."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11: The bias-variance tradeoff\n",
    "By artificially restricting the depth of our tree, we prevent it from creating a complex enough model to correctly categorize some of the rows. If we don't perform the artificial restrictions, the tree becomes too complex, and fits quirks in the data that only exist in the training set, but don't generalize to new data.\n",
    "\n",
    "High bias can cause underfitting -- if a model is consistently failing to predict the correct value, it may be that it is too simple to actually model the data.\n",
    "\n",
    "High variance can cause overfitting -- if a model is very susceptible to small changes in the input data, and changes its predictions massively, then it is likely fitting itself to quirks in the training data, and not making a generalizable model.\n",
    "\n",
    "In general, decision trees suffer from high variance. The whole structure of a decision tree can change if you make a minor alteration to its training data. By restricting the depth of the tree, we increase the bias and decrease the variance. If we restrict the depth too much, we increase bias to the point where it will underfit.\n",
    "\n",
    "Generally, you'll need to use your intuition and manually tweak parameters to get the \"right\" fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12: Exploring decision tree variance\n",
    "We can induce variance and see what happens with a decision tree. To add noise, we'll just add a column of random values.\n",
    "\n",
    " A model with high variance (like a decision tree) will pick up on this noise, and overfit to it. This is because models with high variance are very sensitive to small changes in input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.989559993763 0.697818001302\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "income['noise'] = np.random.randint(4,size=income.shape[0])\n",
    "\n",
    "columns = [\"noise\", \"age\", \"workclass\", \"education_num\", \"marital_status\",\n",
    "           \"occupation\", \"relationship\", \"race\", \"sex\", \"hours_per_week\",\n",
    "           \"native_country\"]\n",
    "\n",
    "train_max_row = int(math.floor(income.shape[0]*.8))\n",
    "train = income.iloc[:train_max_row]\n",
    "test = income.iloc[train_max_row:]\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=1)\n",
    "clf.fit(train[columns],train['high_income'])\n",
    "\n",
    "predictions_train = clf.predict(train[columns])\n",
    "train_auc = roc_auc_score(predictions_train,train['high_income'])\n",
    "\n",
    "predictions_test = clf.predict(test[columns])\n",
    "test_auc = roc_auc_score(predictions_test,test['high_income'])\n",
    "\n",
    "print train_auc,test_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13: Pruning\n",
    "As you can see above, the random noise column causes significant overfitting.Our test set accuracy decreases to .697, and our training set accuracy increases to .989.\n",
    "\n",
    "Another technique is called pruning. Pruning involves building a full tree, and then removing the leaves that don't add to prediction accuracy. Pruning prevents a model from becoming overly complex, and can make a simpler model with higher accuracy on the testing set.\n",
    "\n",
    "Pruning is less commonly used than parameter optimization (like we just did), and ensembling. That's not to say that it isn't an important technique, and we'll cover it in more depth down the line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14: When to use decision trees\n",
    "Let's go over the main advantages and disadvantages of decision trees. The main advantages of decision trees are:\n",
    "\n",
    "- Easy to interpret\n",
    "- Relatively fast to fit and make predictions\n",
    "- Able to handle multiple types of data\n",
    "- Can pick up nonlinearities in data, and are usually fairly accurate\n",
    "\n",
    "The main disadvantage is a tendency to overfit.\n",
    "\n",
    "In tasks where it's important to be able to interpret and convey why the algorithm is doing what it's doing, decision trees are a good choice. \n",
    "\n",
    "The most powerful way to reduce decision tree overfitting is to create ensembles of trees. A popular algorithm to do this is called random forest. We'll cover random forests in the next mission. In cases where prediction accuracy is the most important consideration, random forests usually perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
